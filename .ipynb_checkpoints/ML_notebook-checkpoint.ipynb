{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions:\n",
    "- sigmoid (done)\n",
    "- tanh \n",
    "- relu \n",
    "<br/> \n",
    "Helpful resource: \n",
    "- http://cs231n.github.io/optimization-2/\n",
    "- https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://github.com/joannejc/MachineLearning_Numpy/blob/master/resources/fn%20table.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Sigmoid(object):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.grad = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        f = 1.0/(np.exp(x) + 1)\n",
    "        self.grad = (1 - f) * f\n",
    "        return f\n",
    "    \n",
    "    def backward(self, dx):\n",
    "        ''' dx is downstream gradient, need to compute dx * grad to return to upstream.\n",
    "            Assume dx and stored grad are of the same dimension.\n",
    "        '''\n",
    "        return self.grad * dx\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Test:\n",
    "sig = Sigmoid()\n",
    "np.random.seed(100)\n",
    "x = np.random.randn(5,3)\n",
    "f = sig.forward(x)\n",
    "\n",
    "#x, f, sig.grad, sig.backward(x*x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create linear layer\n",
    "\n",
    "class LinearLayer(object):\n",
    "    ''' affine transform on input x  with parameters w and b: y = wx + b\n",
    "        input_dim is number of variables (dim of x)\n",
    "    '''\n",
    "    def __init__(self, input_dim, output_dim, weight_init = 'randn'):\n",
    "        ### add util to have all diff weight_init methods\n",
    "        ### assume below wt initializing method is what we have for now:\n",
    "        self.w = np.random.randn(input_dim, output_dim) * 1e-2\n",
    "        self.b = np.random.randn(1, output_dim) * 0\n",
    "        self.x = None\n",
    "        \n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        y = np.matmul(x, self.w) + self.b\n",
    "        #print(x.shape, self.w.shape, self.b.shape, y.shape)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        ''' dy is downstream gradient, need to compute dw, db, and  \n",
    "            return dx to upstream.\n",
    "        '''\n",
    "        self.dw = np.matmul(self.x.T,dy)\n",
    "        self.db = np.sum(dy, axis = 0, keepdims= True)\n",
    "        dx = np.matmul(dy, self.w.T)\n",
    "        return dx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5) (5, 3) (1, 3) (10, 3)\n"
     ]
    }
   ],
   "source": [
    "input_dim = 5\n",
    "output_dim = 3\n",
    "x = np.random.randn(10, 5) # 10 examples with 5 variables each\n",
    "layer = LinearLayer(input_dim, output_dim)\n",
    "\n",
    "y = layer.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5) (5, 3) (1, 3) (10, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.0008409 ,  0.02361694, -0.0287744 ],\n",
       "       [-0.0008409 ,  0.02361694, -0.0287744 ],\n",
       "       [-0.0008409 ,  0.02361694, -0.0287744 ],\n",
       "       [-0.0008409 ,  0.02361694, -0.0287744 ],\n",
       "       [-0.0008409 ,  0.02361694, -0.0287744 ],\n",
       "       [-0.0008409 ,  0.02361694, -0.0287744 ],\n",
       "       [-0.0008409 ,  0.02361694, -0.0287744 ],\n",
       "       [-0.0008409 ,  0.02361694, -0.0287744 ],\n",
       "       [-0.0008409 ,  0.02361694, -0.0287744 ],\n",
       "       [-0.0008409 ,  0.02361694, -0.0287744 ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shapes:\n",
    "x = np.ones([10,5])\n",
    "W = np.ones([5,3])\n",
    "b = np.ones([1,3])\n",
    "y = np.matmul(x,W)\n",
    "dy = y*2\n",
    "\n",
    "dW = np.matmul(x.T,dy)\n",
    "dW.shape\n",
    "\n",
    "db = np.matmul(np.ones([1,10]), dy)\n",
    "db.shape\n",
    "np.sum(dy, axis = 0, keepdims= True), db\n",
    "\n",
    "y = layer.forward(x)\n",
    "y\n",
    "#layer.backward(dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MSE(y_pred, y):\n",
    "    ''' Assume y_pred and y have the same dim and can be batched.\n",
    "    '''\n",
    "    b_size = y_pred.shape[0]\n",
    "    diff = y_pred.reshape(b_size,-1) - y.reshape(b_size,-1)\n",
    "    mse = np.power(diff, 2).mean()\n",
    "    \n",
    "    dLoss = 2 * diff # dLoss/dy_pred\n",
    "    \n",
    "    return mse, dLoss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test\n",
    "mse, dLoss = MSE(np.random.rand(10,1), np.random.randn(10,1))\n",
    "mse, dLoss\n",
    "np.random.randn(10).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model:\n",
    "Test model1 with 1 hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before shape: (100, 5)\n",
      "after forward pass shape: (100, 1)\n",
      "(100, 1) (100,)\n",
      "before shape: (100, 1)\n",
      "after forward pass shape: (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Sample Data:\n",
    "x = np.random.randn(100,5)\n",
    "# example: y = x1*x2*x3^3 + x4/x5 - x5\n",
    "y = np.asarray(x[:,0]*x[:,1]*np.power(x[:,2],3) + x[:,3]/x[:,4] - x[:,4])\n",
    "\n",
    "\n",
    "# create our model, for now it's just a list, later i will make it a class\n",
    "\n",
    "model = []\n",
    "\n",
    "# let's add the modules to our model\n",
    "model.append(LinearLayer(5,10)) # add input to hidden layer\n",
    "model.append(Sigmoid()) # add the nonlinear activation\n",
    "model.append(LinearLayer(10,1)) # output layer\n",
    "\n",
    "# Forward pass:\n",
    "print('before shape:',x.shape)\n",
    "\n",
    "for mod in model:\n",
    "    x = mod.forward(x)\n",
    "    \n",
    "print('after forward pass shape:',x.shape)\n",
    "\n",
    "\n",
    "# Loss:\n",
    "print(x.shape, y.shape)\n",
    "mse, dLoss = MSE(x, y)\n",
    "dx = dLoss\n",
    "\n",
    "\n",
    "# Backward pass:\n",
    "print('before shape:',x.shape)\n",
    "\n",
    "for mod in reversed(model):\n",
    "    dx = mod.backward(dx)\n",
    "    \n",
    "print('after forward pass shape:',x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse, dLoss = MSE(x, y)\n",
    "dLoss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "# Create hidden layer 1:\n",
    "input_dim1 = x.shape[1] #5\n",
    "layer1 = LinearLayer(5, 10)\n",
    "\n",
    "# hidden layer 2:\n",
    "layer2 = LinearLayer(10, 1)\n",
    "\n",
    "# Create activation fn:\n",
    "sig = Sigmoid()\n",
    "\n",
    "# Model:\n",
    "x1 = sig.forward(x)\n",
    "y1 = layer1.forward(x1)\n",
    "y_pred.shape\n",
    "#mse, dLoss = MSE(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(X, W):\n",
    "    '''\n",
    "    Computes and returns output of sigmoid fn (f), dX, dW\n",
    "        Args:\n",
    "            X: inputs\n",
    "            W: weights\n",
    "            X, W should be numpy arrays, could be either matrices or vectors of the same size\n",
    "    '''\n",
    "    \n",
    "    # Forward pass:\n",
    "    D = W.dot(X)\n",
    "    f = 1.0/(np.exp(D) + 1)\n",
    "    \n",
    "    # Backpropagation:\n",
    "    dD = np.asarray((1 - f) * f) # gradient of dot variable (D)\n",
    "    dX = W.T.dot(dD) # backprop into X\n",
    "    dW = dD.dot(X.T) # backprop into W\n",
    "    \n",
    "    return  f, dX, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test:\n",
    "# matrix-matrix test:\n",
    "np.random.seed(100)\n",
    "W = np.random.randn(5,10)\n",
    "X = np.random.randn(10,3)\n",
    "\n",
    "F, dX, dW = sigmoid(X, W)\n",
    "\n",
    "# vector-vector test:\n",
    "w = [2,-3]\n",
    "x = [-1, -2]\n",
    "x = np.asarray(x)\n",
    "w = np.asarray(w)\n",
    "f, dx, dw = sigmoid(x,w)\n",
    "\n",
    "f, F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tanh(X, W):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Pytorch0.4",
   "language": "python",
   "name": "pytorch0.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
